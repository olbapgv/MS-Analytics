{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"notebook-word2vec.ipynb","provenance":[],"collapsed_sections":["WE4yoVgwGsq-","lAwkTPnhwZKC","-mjITvDdq8JX"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Oh5Q4mf7FwMd"},"source":["# Fake News Detection: A social media approach"]},{"cell_type":"markdown","metadata":{"id":"whGCTUVZF0et"},"source":["## Package set up"]},{"cell_type":"code","metadata":{"id":"T9MJthyuNc3U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627821205096,"user_tz":-600,"elapsed":23480,"user":{"displayName":"Jeffrey Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gghwcrlq7Bg5z2FIUPZ_FpYFvZVj7jL2r4qB-tWQqg=s64","userId":"05825880180168674320"}},"outputId":"c8ef0888-68f5-49f6-e751-174b9bed8e80"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2jfeB289WVQa"},"source":["user = \"pablo\" #\"jeff\"\n","\n","# set up\n","pip_install_lib = True\n","\n","# Preprocessing\n","preprocess = False\n","delete_networks = False # Only required when no pre-processing is done\n","\n","# Word2Vec embedding\n","load_word2vec_model = True\n","word2vec_type = 'skipgram' # skipgram or cbow\n","load_tokenised_inputs = True\n","\n","# Transformers\n","load_roberta = True\n","load_distilbert = True\n","\n","# Doc2Vec embedding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GuYLrtseNIrP"},"source":["# set paths\n","import os\n","\n","if user == \"jeff\":\n","  path = r\"/content/drive/MyDrive/Colab Notebooks/cse7643 group project/Final Project\"\n","  module_path = r\"/content/drive/MyDrive/Colab Notebooks/cse7643 group project/Final Project/utils/\"\n","  model_path = r\"/content/drive/MyDrive/Colab Notebooks/cse7643 group project/Final Project/models/\"\n","  data_path = r\"/content/drive/MyDrive/Colab Notebooks/cse7643 group project/Final Project/data/\"\n","else:\n","  path = r\"/content/drive/MyDrive/Gatech/Deep Learning/Final Project\"\n","  module_path = r\"/content/drive/MyDrive/Gatech/Deep Learning/Final Project/utils/\"\n","  model_path = r\"/content/drive/MyDrive/Gatech/Deep Learning/Final Project/models/\"\n","\n","os.chdir(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bp9IZoeGGGWJ"},"source":["%%capture\n","if pip_install_lib:\n","  ! pip install 'fsspec>=0.3.3'\n","  ! pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XWf5AGUS8N3c"},"source":["%%capture\n","import pandas as pd\n","from gensim.models import Word2Vec\n","from gensim.utils import tokenize\n","from gensim.matutils import cossim\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","import dask.dataframe as dd\n","from dask.diagnostics import ProgressBar\n","from tqdm.notebook import tqdm\n","from joblib import Parallel, delayed\n","import sys\n","from sklearn.feature_extraction.text import CountVectorizer\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","import pickle\n","from sklearn.metrics import accuracy_score\n","import nltk\n","import re\n","from nltk.corpus import stopwords\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","from IPython.core.display import HTML # Let's center our outputs\n","HTML(\"\"\"\n","<style>\n",".output_png {\n","    display: table-cell;\n","    text-align: center;\n","    vertical-align: middle;\n","}\n",".dataframe {\n","    margin-left: auto !important;\n","    margin-right: auto !important;\n","}\n","\n","</style>\n","\"\"\")\n","\n","if module_path not in sys.path:\n","    sys.path.append(module_path)\n","if model_path not in sys.path:\n","    sys.path.append(model_path)\n","\n","import general\n","from loader import Loader\n","from models.BILSTM import BILSTM #BILSTM(input_dim, batch_size)\n","from models.MLP import MLP #MLP(input_dim)\n","from models.MLP2 import MLP2 #MLP2(input_dim)\n","from models.BILSTM2 import BILSTM2 #BILSTM2(input_dim, batch_size)\n","from models.EMB_BILSTM import EMB_BILSTM #EMB_BILSTM(batch_size, weights)\n","from models.EMB_CNN import EMB_CNN #EMB_CNN(weights)\n","from models.DistilBert import DistilBert\n","from models.Roberta import roberta\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mAY_OYoBJgjZ"},"source":["%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WE4yoVgwGsq-"},"source":["## Exploratory Data Analysis"]},{"cell_type":"code","metadata":{"id":"NdZ8nEIaGVUu"},"source":["# Step 1: We first read the data, concatenate our fake and true news and tag them as such.\n","# Additionally, title and text (content) are joined in order to make us of all this inforamtion.\n","# Once we have generated the required dataframe, we print 5 observations to observe the result.\n","df = general.read_data()\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vUeK7T4HRcFQ"},"source":["When analyzing the data set, one of our first questions consists in whether our classification problem has data imbalance. For this purpose, we first plot the proportion of Fake and True news, concluding that we should not expect any imbalance related problem during the project."]},{"cell_type":"code","metadata":{"id":"n8Xz0cbBNcC5"},"source":["# We can notice that the data is rather balanced and we should not expect any imbalance related problem\n","df.label.value_counts().plot.pie(autopct=\"%.1f%%\", ylabel = '', title = \"Proportion of fake news\", \n","                                textprops={'color':\"w\"})\n","plt.legend(['Real News','Fake News'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"auzKHy3QR3VI"},"source":["Another important question we would like to know is: how is our data composed in terms of subjects. We can notice that there is not a clear differentiation between the classes for true and fake news. Thus, we conclude that it is better not to include this information as an independent variable."]},{"cell_type":"code","metadata":{"id":"KyGcpv2PVhYW"},"source":["count = sns.catplot(x=\"label\", hue=\"subject\",\n","                data=df, kind=\"count\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3LR91pmWbp3"},"source":["# We notice that there is a considerable amount of news with duplicated content\n","print(f\"{sum( df.duplicated('text') )} observations were duplicates\")\n","# Thus, we can safely remove these observations as they add no information\n","df = df.drop_duplicates(subset='text', keep='first')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5aTMXlPUWNR8"},"source":["# The following plot shows the amount of fake and real news per day.\n","general.plot_count(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xTH9ghuA3Rb6"},"source":["# Finally, when reading some news, we noticed that some of them included the source\n","# When checking, we noticed that the presence of the word Reuters, alone, could \n","# predict approximately 99% of the cases, which is not useful for our intention \n","# of creating a fake news detector that could eventually generalize to other datasets.\n","# https://today.yougov.com/ratings/media/popularity/news-websites/all\n","proportions = []\n","networks = ['cnn', 'reuters', 'times', 'bloomberg','bbc','forbes','abc','fox',\n","            'buzzfeed','cbs','huffington','msn','nbc','cnbc','yahoo','google']\n","\n","source_dict = {}\n","for i in networks:\n","  source_dict[i] = 100*round(df.loc[df[\"text\"].str.contains(i),\"label\"].mean(),2)\n","\n","table = pd.DataFrame.from_dict(source_dict, orient=\"index\").reset_index()\n","table.columns = ['source','proportion of fake news (%)']\n","table"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YgIRAK5VjiWu"},"source":["## Preprocessing"]},{"cell_type":"code","metadata":{"id":"JU_kK1pJWsHe"},"source":["# As previously indicated, we have noticed that both the date and subject are not generalizable\n","# And therefore we will delete these variables as well as the generated ones.\n","df = df.drop(['subject','date'], axis = \"columns\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"odGm79_4nvPb"},"source":["# apply preprocessing if specified\n","if preprocess == True:\n","\n","  # Stopwords\n","  stop = np.append( stopwords.words('english') , networks )\n","\n","  def del_sw(word, stop):\n","      return \"\" if word in stop or len(word) <= 3 else word\n","\n","  def lem(word):\n","    output = [nltk.WordNetLemmatizer().lemmatize(i) for i in word]\n","    return output\n","\n","  df.text = df.text.apply(lambda x: \" \".join([del_sw(i,stop) for i in re.split(r'\\W+', x)]))\n","  df.text = df.text.apply(lambda x: \" \".join([nltk.WordNetLemmatizer().lemmatize(i) for i in str(x).split()]))\n","\n","elif delete_networks == True:\n","  def del_net(word, stop):\n","    return \"\" if word in stop else word  \n","  df.text = df.text.apply(lambda x: \" \".join([del_net(i,networks) for i in x.split() ]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6XQ_tgebMT7U"},"source":["# Finally, we split the data set into a training and a test set\n","X, y = df[\"text\"], df[\"label\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True, stratify=y)\n","X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=1/9, random_state=42, shuffle=True, stratify=y_train)\n","\n","y_train, y_valid, y_test = y_train.values, y_valid.values, y_test.values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2DyiZcImfOZ-"},"source":["## Create embeddings for news articles using Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"Nh8_5LPLosCG"},"source":["### Train word2vec model"]},{"cell_type":"code","metadata":{"id":"BUFtR2OHfUeQ"},"source":["if load_word2vec_model == False:\n","  if word2vec_type == 'skipgram':\n","    # generate list of tokenized articles\n","    sent = X_train.apply(lambda x: x.split(\" \"))\n","    # build word2vec model\n","    word2vec_model = Word2Vec(\n","        sent\n","        ,min_count=1 # 100\n","        ,size= 100\n","        ,workers=8\n","        ,window =3\n","        ,sg = 1 # skip-gram\n","    )\n","    word2vec_model.save(\"models/word2vec.model\")\n","  else:\n","    # generate list of tokenized articles\n","    sent = X_train.apply(lambda x: x.split(\" \"))\n","    # build word2vec model\n","    word2vec_model = Word2Vec(\n","        sent\n","        ,min_count=1 # 100\n","        ,size= 100\n","        ,workers=8\n","        ,window =3\n","        ,sg = 0 # CBOW\n","    )\n","    word2vec_model.save(\"models/word2vec_cbow.model\")\n","\n","else:\n","  if word2vec_type == 'skipgram':\n","    word2vec_model = Word2Vec.load(\"models/word2vec.model\")\n","  else:\n","    word2vec_model = Word2Vec.load(\"models/word2vec_cbow.model\")  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T0-qLKSCwh73"},"source":["words = word2vec_model.wv.index2word\n","word2idx = {o:i for i,o in enumerate(words)}\n","idx2word = {i:o for i,o in enumerate(words)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t3eP8nHrouXd"},"source":["### Tokenize inputs"]},{"cell_type":"code","metadata":{"id":"AWj7ZZwegmrO"},"source":["# tokenize inputs\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","if load_tokenised_inputs == False:\n","\n","  tokenizer = Tokenizer()\n","  tokenizer.fit_on_texts(X_train)\n","\n","  X_train = tokenizer.texts_to_sequences(X_train)\n","  X_valid = tokenizer.texts_to_sequences(X_valid)\n","  X_test = tokenizer.texts_to_sequences(X_test)\n","\n","  maxlen = 700\n","\n","  #Making all news of size maxlen defined above\n","  X_train = pad_sequences(X_train, maxlen=maxlen)\n","  X_valid = pad_sequences(X_valid, maxlen=maxlen)\n","  X_test = pad_sequences(X_test, maxlen=maxlen)\n","\n","  pickle.dump(X_train, open(\"data/X_train.pickle\", \"wb\"))\n","  pickle.dump(X_valid, open(\"data/X_valid.pickle\", \"wb\"))\n","  pickle.dump(X_test, open(\"data/X_test.pickle\", \"wb\"))\n","  pickle.dump(y_train, open(\"data/y_train.pickle\", \"wb\"))\n","  pickle.dump(y_valid, open(\"data/y_valid.pickle\", \"wb\"))\n","  pickle.dump(y_test, open(\"data/y_test.pickle\", \"wb\"))\n","\n","else:\n","\n","  # load in embedded document vectors\n","  X_train = pickle.load(open(\"data/X_train.pickle\",\"rb\"))\n","  X_valid = pickle.load(open(\"data/X_valid.pickle\",\"rb\"))\n","  X_test = pickle.load(open(\"data/X_test.pickle\",\"rb\"))\n","  y_train = pickle.load(open(\"data/y_train.pickle\",\"rb\"))\n","  y_valid = pickle.load(open(\"data/y_valid.pickle\",\"rb\"))\n","  y_test = pickle.load(open(\"data/y_test.pickle\",\"rb\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"im9l8akuoxPN"},"source":["### Create embedding"]},{"cell_type":"code","metadata":{"id":"L3qJ3-CnhrsG"},"source":["weights = torch.FloatTensor(word2vec_model.wv.vectors)\n","embedding = nn.Embedding.from_pretrained(weights)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aCtLua9euTOu"},"source":["## Networks with Embeddings\n"]},{"cell_type":"markdown","metadata":{"id":"jdOqNjIPglrD"},"source":["### CNN"]},{"cell_type":"code","metadata":{"id":"1I2KJVxuhAdM"},"source":["# convert to tensors\n","X_train_tensor = torch.tensor(X_train).to(int)\n","y_train_tensor = torch.tensor(y_train)\n","train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n","\n","X_valid_tensor = torch.tensor(X_valid).to(int)\n","y_valid_tensor = torch.tensor(y_valid)\n","valid_dataset = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n","\n","X_test_tensor = torch.tensor(X_test).to(int)\n","y_test_tensor = torch.tensor(y_test)\n","test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDeb-g8ogwrJ"},"source":["# Hyper-parameters\n","input_dim = int(X_train_tensor.shape[1])\n","batch_size = 128\n","\n","# Loader\n","mloader = Loader(train_dataset, valid_dataset, batch_size)\n","mloader.train(model = EMB_CNN(weights) , verbose = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwFe_yF8g1dl"},"source":["mloader.eval( test_dataset )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRYHQO2Dp6IU"},"source":["### LSTM model"]},{"cell_type":"code","metadata":{"id":"DifHitywp01_"},"source":["# convert to tensors\n","X_train_tensor = torch.tensor(X_train).to(int)\n","y_train_tensor = torch.tensor(y_train)\n","train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n","\n","X_valid_tensor = torch.tensor(X_valid).to(int)\n","y_valid_tensor = torch.tensor(y_valid)\n","valid_dataset = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n","\n","X_test_tensor = torch.tensor(X_test).to(int)\n","y_test_tensor = torch.tensor(y_test)\n","test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBp__9jkVCug"},"source":["# Hyper-parameters\n","input_dim = int(X_train_tensor.shape[1])\n","batch_size = 128\n","\n","# Loader\n","mloader = Loader(train_dataset, valid_dataset, batch_size)\n","mloader.train(model = EMB_BILSTM(batch_size, weights) , verbose = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0bLCu8k-vY5P"},"source":["mloader.eval( test_dataset )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"raD8EdHxvP6B"},"source":["# class Model(nn.Module):\n","#     def __init__(self, embedding, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n","#         super(Model, self).__init__()\n","#         self.output_size = output_size\n","#         self.n_layers = n_layers\n","#         self.hidden_dim = hidden_dim\n","        \n","#         self.embedding = embedding\n","#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n","#         self.dropout = nn.Dropout(drop_prob)\n","#         self.fc = nn.Linear(hidden_dim, output_size)\n","#         self.sigmoid = nn.Sigmoid()\n","        \n","#     def forward(self, x, hidden):\n","#         batch_size = x.size(0)\n","#         x = x.long()\n","#         embeds = self.embedding(x)\n","#         lstm_out, hidden = self.lstm(embeds, hidden)\n","#         lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n","        \n","#         out = self.dropout(lstm_out)\n","#         out = self.fc(out)\n","#         out = self.sigmoid(out)\n","        \n","#         out = out.view(batch_size, -1)\n","#         out = out[:,-1]\n","#         return out, hidden\n","    \n","#     def init_hidden(self, batch_size):\n","#         weight = next(self.parameters()).data\n","#         hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n","#                       weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n","#         return hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sf_6-N7xyl-c"},"source":["# vocab_size = len(word2idx) + 1\n","# output_size = 1\n","# embedding_dim = 100\n","# hidden_dim = 512\n","# n_layers = 2\n","# model = Model(embedding, vocab_size, output_size, embedding_dim, hidden_dim, n_layers).to(device)\n","\n","\n","# optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n","# loss_func = nn.BCELoss()\n","\n","# num_epoch = 10\n","\n","# # start training\n","# # lists for storing loss at each epoch\n","# best_val_loss = 99999999 # store best validation loss\n","# epoch_list = [] # list of epochs (for plotting later)\n","# train_loss_list = [] # training loss at each epoch\n","# val_loss_list = [] # validation loss at each epoch\n","\n","# for epoch in range(num_epoch): # loop through epoc\n","#     model.train() # set model to training mode\n","#     batch_losses = []\n","\n","#     h = model.init_hidden(batch_size)\n","\n","#     for batch_x, batch_y in train_loader: # for each training step\n","        \n","#         # get batch of data\n","#         train = torch.autograd.Variable(batch_x).float().to(device)\n","#         label = torch.autograd.Variable(batch_y).float().to(device)\n","\n","#         # h = tuple([e.data for e in h])\n","#         prediction = model(train, h).to(device) # forward propogration\n","#         label = torch.nn.functional.one_hot(label.to(torch.int64), 2)\n","#         loss = loss_func(prediction, label.float().detach()) # calculate loss\n","#         loss.backward() # calculate gradients\n","#         optimizer.step() # update parameters based on caluclated gradients\n","#         optimizer.zero_grad() # clear gradients for next train\n","\n","#         batch_losses.append(loss.item()) # record loss for this mini batch\n","\n","#     # record average mini batch loss over epoch\n","#     training_loss = np.mean(batch_losses)\n","#     train_loss_list.append(training_loss)\n","\n","#     with torch.no_grad(): # set all requires_grad to false\n","#         val_losses = []\n","#         for batch_x, batch_y in valid_loader:\n","#             model.eval() # ensure batchnorm and dropout work properly in evaluation mode\n","\n","#             batch_x = batch_x.float().to(device)\n","#             batch_y = batch_y.float().to(device)\n","\n","#             yhat = model(batch_x).to(device)\n","\n","#             # record loss for this mini batch\n","#             batch_y = torch.nn.functional.one_hot(batch_y.to(torch.int64), 2)\n","#             val_loss = loss_func(yhat, batch_y.float().detach()).item()\n","#             val_losses.append(val_loss)\n","        \n","#         # record average mini batch loss for validation\n","#         validation_loss = np.mean(val_losses)\n","#         if validation_loss < best_val_loss: best_val_loss = validation_loss # record best validation loss\n","#         val_loss_list.append(validation_loss)\n","\n","            \n","#     print(f\"[{epoch+1}] Training loss: {training_loss:.5f}\\t Validation loss: {validation_loss:.5f}\\t Best Validation loss: {best_val_loss:.5f}\")\n","#     epoch_list.append(epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAwkTPnhwZKC"},"source":["### Roberta"]},{"cell_type":"code","metadata":{"id":"cnA28ySdwcoe"},"source":["# Finally, we split the data set into a training and a test set\n","X, y = df[\"text\"], df[\"label\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True, stratify=y)\n","X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=1/9, random_state=42, shuffle=True, stratify=y_train)\n","\n","y_train, y_valid, y_test = y_train.values, y_valid.values, y_test.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WEbrvklPweTx"},"source":["%%capture\n","\n","if load_roberta == False:\n","  tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","  X_train_tensor = general.autok(X_train, tokenizer)\n","  y_train_tensor = torch.tensor(y_train)\n","  train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n","  X_valid_tensor = general.autok(X_valid, tokenizer)\n","  y_valid_tensor = torch.tensor(y_valid)\n","  valid_dataset = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n","  X_test_tensor = general.autok(X_test, tokenizer)\n","  y_test_tensor = torch.tensor(y_test)\n","  test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n","  torch.save(X_train_tensor, 'data/transformers/Rob_X_train_tensor.pt')\n","  torch.save(y_train_tensor, 'data/transformers/Rob_y_train_tensor.pt')\n","  torch.save(X_valid_tensor, 'data/transformers/Rob_X_valid_tensor.pt')\n","  torch.save(y_valid_tensor, 'data/transformers/Rob_y_valid_tensor.pt')\n","  torch.save(X_test_tensor, 'data/transformers/Rob_X_test_tensor.pt')\n","  torch.save(y_test_tensor, 'data/transformers/Rob_y_test_tensor.pt')\n","else:\n","  X_train_tensor = torch.load('data/transformers/Rob_X_train_tensor.pt')\n","  y_train_tensor = torch.load('data/transformers/Rob_y_train_tensor.pt')\n","  X_valid_tensor = torch.load('data/transformers/Rob_X_valid_tensor.pt')\n","  y_valid_tensor = torch.load('data/transformers/Rob_y_valid_tensor.pt')\n","  X_test_tensor = torch.load('data/transformers/Rob_X_test_tensor.pt')\n","  y_test_tensor = torch.load('data/transformers/Rob_y_test_tensor.pt')\n","  train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n","  valid_dataset = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n","  test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kpdu7pwCw64H"},"source":["# Loader\n","mloader = Loader(train_dataset, valid_dataset, batch_size = 128)\n","mloader.train(model = roberta() , verbose = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kZ_apsChw-dw"},"source":["mloader.eval( test_dataset )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-mjITvDdq8JX"},"source":["### Distilbert"]},{"cell_type":"code","metadata":{"id":"qItlDywch5yE"},"source":["# Finally, we split the data set into a training and a test set\n","X, y = df[\"text\"], df[\"label\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True, stratify=y)\n","X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=1/9, random_state=42, shuffle=True, stratify=y_train)\n","\n","y_train, y_valid, y_test = y_train.values, y_valid.values, y_test.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xCayKnoMnVZd"},"source":["%%capture\n","\n","if load_distilbert == False:\n","  tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","  X_train_tensor = general.autok(X_train, tokenizer)\n","  y_train_tensor = torch.tensor(y_train)\n","  train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n","  X_valid_tensor = general.autok(X_valid, tokenizer)\n","  y_valid_tensor = torch.tensor(y_valid)\n","  valid_dataset = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n","  X_test_tensor = general.autok(X_test, tokenizer)\n","  y_test_tensor = torch.tensor(y_test)\n","  test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n","  torch.save(X_train_tensor, 'data/transformers/DB_X_train_tensor.pt')\n","  torch.save(y_train_tensor, 'data/transformers/DB_y_train_tensor.pt')\n","  torch.save(X_valid_tensor, 'data/transformers/DB_X_valid_tensor.pt')\n","  torch.save(y_valid_tensor, 'data/transformers/DB_y_valid_tensor.pt')\n","  torch.save(X_test_tensor, 'data/transformers/DB_X_test_tensor.pt')\n","  torch.save(y_test_tensor, 'data/transformers/DB_y_test_tensor.pt')\n","else:\n","  X_train_tensor = torch.load('data/transformers/DB_X_train_tensor.pt')\n","  y_train_tensor = torch.load('data/transformers/DB_y_train_tensor.pt')\n","  X_valid_tensor = torch.load('data/transformers/DB_X_valid_tensor.pt')\n","  y_valid_tensor = torch.load('data/transformers/DB_y_valid_tensor.pt')\n","  X_test_tensor = torch.load('data/transformers/DB_X_test_tensor.pt')\n","  y_test_tensor = torch.load('data/transformers/DB_y_test_tensor.pt')\n","  train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n","  valid_dataset = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n","  test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FC-QcCI4lgHM"},"source":["# Loader\n","mloader = Loader(train_dataset, valid_dataset, batch_size = 128)\n","mloader.train(model = DistilBert() , verbose = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZU4Fn87_qz-v"},"source":["mloader.eval( test_dataset )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jCiXFqDYHfZJ"},"source":["## Create embeddings for news articles using doc2vec"]},{"cell_type":"code","metadata":{"id":"awQKyyizGVXJ"},"source":["# create a column that has a unique id for each document - doc2vec needs a tag\n","X_train_index = X_train.reset_index(drop=True).reset_index()\n","\n","# tag and tokenise documents for doc2vec model input\n","X_train_tagged = general.get_tagged_documents(X_train_index, 'text', ['index'])\n","\n","# generate doc2vec model\n","doc2vec_model = general.generate_doc2vec_model(\n","    X_train_tagged\n","    ,method=\"dbow\"\n","    ,max_epochs=10\n","    ,vec_size=50\n","    ,alpha=0.025\n","    ,min_count=50\n",")\n","\n","# infer vectors using trained model\n","X_train_vector = general.get_vector_representations(doc2vec_model, X_train)\n","X_valid_vector = general.get_vector_representations(doc2vec_model, X_valid)\n","X_test_vector = general.get_vector_representations(doc2vec_model, X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvmSDO8IGVcN"},"source":["# # pickle the files so we can load it in later\n","# os.chdir(data_path)\n","# pickle.dump(X_train_vector, open(\"X_train_vector_stopwords_True_doc2vec_PVDM_dim_50.pickle\", \"wb\"))\n","# pickle.dump(X_valid_vector, open(\"X_valid_vector_stopwords_True_doc2vec_PVDM_dim_50.pickle\", \"wb\"))\n","# pickle.dump(X_test_vector, open(\"X_test_vector_stopwords_True_doc2vec_PVDM_dim_50.pickle\", \"wb\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YCmDyG9Y-hG6"},"source":["# # pickle the files so we can load it in later\n","# os.chdir(data_path)\n","# pickle.dump(X_train_vector, open(\"X_train_vector_stopwords_True_doc2vec_DBOW_dim_50.pickle\", \"wb\"))\n","# pickle.dump(X_valid_vector, open(\"X_valid_vector_stopwords_True_doc2vec_DBOW_dim_50.pickle\", \"wb\"))\n","# pickle.dump(X_test_vector, open(\"X_test_vector_stopwords_True_doc2vec_DBOW_dim_50.pickle\", \"wb\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F1QMWLQZFhow"},"source":["# # pickle the files so we can load it in later\n","# os.chdir(data_path)\n","# pickle.dump(X_train_vector, open(\"X_train_vector_stopwords_True_doc2vec_DBOW_dim_100.pickle\", \"wb\"))\n","# pickle.dump(X_valid_vector, open(\"X_valid_vector_stopwords_True_doc2vec_DBOW_dim_100.pickle\", \"wb\"))\n","# pickle.dump(X_test_vector, open(\"X_test_vector_stopwords_True_doc2vec_DBOW_dim_100.pickle\", \"wb\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SHg19dNwJTgy"},"source":["# # pickle the files so we can load it in later\n","# os.chdir(data_path)\n","# pickle.dump(X_train_vector, open(\"X_train_vector_stopwords_False_doc2vec_DBOW_dim_100.pickle\", \"wb\"))\n","# pickle.dump(X_valid_vector, open(\"X_valid_vector_stopwords_False_doc2vec_DBOW_dim_100.pickle\", \"wb\"))\n","# pickle.dump(X_test_vector, open(\"X_test_vector_stopwords_False_doc2vec_DBOW_dim_100.pickle\", \"wb\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eJpDvTGkR0af"},"source":["## MLP"]},{"cell_type":"code","metadata":{"id":"mRAhoOQ_GVeo"},"source":["# load in embedded document vectors\n","# refer to data folder for pickle names\n","X_train_vector_pickle = \"data/X_train_vector_stopwords_False_doc2vec_DBOW_dim_100.pickle\"\n","X_valid_vector_pickle = \"data/X_valid_vector_stopwords_False_doc2vec_DBOW_dim_100.pickle\"\n","X_test_vector_pickle = \"data/X_test_vector_stopwords_False_doc2vec_DBOW_dim_100.pickle\"\n","\n","X_train_vector = pickle.load(open(X_train_vector_pickle,\"rb\"))\n","X_valid_vector = pickle.load(open(X_valid_vector_pickle,\"rb\"))\n","X_test_vector = pickle.load(open(X_test_vector_pickle,\"rb\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"01vLFI3MLTUp"},"source":["# convert to tensors\n","X_train_tensor = torch.tensor(X_train_vector.values).to(float)\n","y_train_tensor = torch.tensor(y_train)\n","train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n","\n","X_valid_tensor = torch.tensor(X_valid_vector.values).to(float)\n","y_valid_tensor = torch.tensor(y_valid)\n","valid_dataset = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n","\n","X_test_tensor = torch.tensor(X_test_vector.values).to(float)\n","y_test_tensor = torch.tensor(y_test)\n","test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TozKqhjCbMMV"},"source":["# Hyper-parameters\n","input_dim = int(X_train_tensor.shape[1])\n","batch_size = 128"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v_SOLn2Gjt-U"},"source":["# Hyper-parameters\n","input_dim = int(X_train_tensor.shape[1])\n","batch_size = 128\n","\n","# Loader\n","mloader = Loader(train_dataset, valid_dataset, batch_size)\n","mloader.train(model = MLP2(input_dim) , verbose = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gx5_Kt7qjvfC"},"source":["mloader.eval( test_dataset )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0z_kEGkyV6q"},"source":["plt.plot(epoch_list, val_loss_list, label=\"validation\")\n","plt.plot(epoch_list, train_loss_list, label=\"train\")\n","plt.ylim(bottom=0)\n","plt.xlabel(\"Number of epochs\")\n","plt.ylabel(\"MSE\")\n","plt.title(\"MLP: BCELoss vs Number of iterations\")\n","plt.legend();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u7F3eweAtVPA"},"source":["***"]}]}